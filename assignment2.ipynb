{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BSjph51BtmC",
        "outputId": "fa72a693-0c0d-4c08-f186-0460dcad5702"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e0b50350470>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Tuple\n",
        "import time\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colab drive configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5VwlrPwbAfW",
        "outputId": "6c94dc97-6970-46c7-df4f-0f6fea320b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Set up logging with a fixed filename\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    log_filename = \"/content/drive/My Drive/training_log.log\"\n",
        "else:\n",
        "    log_filename = \"training_log.log\"\n",
        "#logging.basicConfig(filename=log_filename, level=logging.DEBUG, format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eG7Z0xhB9YD"
      },
      "source": [
        "# Data Preprocessing and Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2n9umTWFCCgl"
      },
      "outputs": [],
      "source": [
        "class CharacterDataset(Dataset):\n",
        "    def __init__(self, data: List[str], vocab: dict, seq_len: int):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        input_seq = self.data[idx:idx + self.seq_len]\n",
        "        target_seq = self.data[idx + 1:idx + self.seq_len + 1]\n",
        "\n",
        "        input_ids = [self.vocab.get(char, self.vocab['[UNK]']) for char in input_seq]\n",
        "        target_ids = [self.vocab.get(char, self.vocab['[UNK]']) for char in target_seq]\n",
        "\n",
        "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
        "\n",
        "def build_vocab(data: str) -> dict:\n",
        "    unique_chars = sorted(set(data))\n",
        "    vocab = {char: idx for idx, char in enumerate(unique_chars, 4)}  # Start at 4 to account for special tokens\n",
        "    vocab['[UNK]'] = 0\n",
        "    vocab['[PAD]'] = 1\n",
        "    vocab['[SOS]'] = 2\n",
        "    vocab['[EOS]'] = 3\n",
        "    return vocab\n",
        "\n",
        "def load_data(file_path: str, max_data_size: int = None) -> str:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = f.read()\n",
        "            if max_data_size is not None:\n",
        "                data = data[:max_data_size]  # Limit the amount of data loaded\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"File not found: {file_path}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "def prepare_datasets(train_file: str, val_file: str, test_file: str, seq_len: int, max_data_size: int = None):\n",
        "    train_data = load_data(train_file, max_data_size)\n",
        "    val_data = load_data(val_file, max_data_size)\n",
        "    test_data = load_data(test_file, max_data_size)\n",
        "\n",
        "    vocab = build_vocab(train_data + val_data + test_data)\n",
        "\n",
        "    train_dataset = CharacterDataset(train_data, vocab, seq_len)\n",
        "    val_dataset = CharacterDataset(val_data, vocab, seq_len)\n",
        "    test_dataset = CharacterDataset(test_data, vocab, seq_len)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLBfme_GCFgx"
      },
      "source": [
        "# Transformer Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GGrzpZZOCJNl"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiQueryAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size, seq_len, _ = q.size()\n",
        "\n",
        "        q = self.q_proj(q).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.k_proj(k).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.v_proj(v).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, block_size):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size, seq_len, _ = q.size()\n",
        "\n",
        "        q = self.q_proj(q).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.k_proj(k).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.v_proj(v).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.zeros_like(q @ k.transpose(-2, -1))\n",
        "        for i in range(0, seq_len, self.block_size):\n",
        "            scores[:, :, i:i+self.block_size] = q[:, :, i:i+self.block_size] @ k[:, :, i:i+self.block_size].transpose(-2, -1)\n",
        "        scores /= np.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, layer_norm_after=True, attention_type='default', block_size=32):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.layer_norm_after = layer_norm_after\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        # Define transformer layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Initialize attention mechanisms based on the attention type\n",
        "        if attention_type == 'multi-query':\n",
        "            self.attention = MultiQueryAttention(d_model, nhead)\n",
        "        elif attention_type == 'sparse':\n",
        "            self.attention = SparseAttention(d_model, nhead, block_size)\n",
        "        else:\n",
        "            self.attention = None  # Default attention (standard PyTorch Transformer)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Layer normalization\n",
        "        if layer_norm_after:\n",
        "            self.layer_norm_encoder = nn.LayerNorm(d_model)\n",
        "            self.layer_norm_decoder = nn.LayerNorm(d_model)\n",
        "        else:\n",
        "            self.layer_norm_encoder = None\n",
        "            self.layer_norm_decoder = None\n",
        "\n",
        "        # Weight tying\n",
        "        self.fc_out.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
        "        tgt = self.embedding(tgt) * np.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src)\n",
        "            if self.layer_norm_after and self.layer_norm_encoder is not None:\n",
        "                src = self.layer_norm_encoder(src)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, src)\n",
        "            if self.layer_norm_after and self.layer_norm_decoder is not None:\n",
        "                tgt = self.layer_norm_decoder(tgt)\n",
        "\n",
        "        # Apply alternative attention if specified\n",
        "        if self.attention_type == 'multi-query':\n",
        "            tgt = self.attention(tgt, src, src)\n",
        "        elif self.attention_type == 'sparse':\n",
        "            tgt = self.attention(tgt, src, src)\n",
        "\n",
        "        output = self.fc_out(tgt)\n",
        "        return output\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src, tgt = zip(*batch)  # Unzip the batch into src and tgt\n",
        "    src = pad_sequence(src, batch_first=True, padding_value=1)  # Pad src sequences\n",
        "    tgt = pad_sequence(tgt, batch_first=True, padding_value=1)  # Pad tgt sequences\n",
        "    return src, tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEoSioDyCMQt"
      },
      "source": [
        "# Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fh3Wkk2pCRuT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device, vocab, epoch, log_interval):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch}\", leave=False)\n",
        "\n",
        "    for batch_idx, (src, tgt) in enumerate(progress_bar):\n",
        "        src, tgt = src.to(device), tgt.to(device)  # Move src and tgt to the same device\n",
        "        optimizer.zero_grad()\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "        tgt_input = pad_sequence([torch.cat((torch.tensor([vocab['[SOS]']]).to(device), x), dim=0) for x in tgt_input],\n",
        "                                 batch_first=True, padding_value=vocab['[PAD]'])\n",
        "        tgt_output = pad_sequence([torch.cat((x, torch.tensor([vocab['[EOS]']]).to(device)), dim=0) for x in tgt_output],\n",
        "                                 batch_first=True, padding_value=vocab['[PAD]'])\n",
        "        output = model(src, tgt_input)\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), tgt_output.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Log training progress\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            ms_per_batch = elapsed * 1000 / log_interval\n",
        "            loss_value = total_loss / (batch_idx + 1)\n",
        "            bpc = calculate_bpc(loss_value)\n",
        "            progress_bar.set_postfix(loss=loss_value, bpc=bpc)\n",
        "            #logging.info(f\"| epoch {epoch} | {batch_idx + 1:5d}/{num_batches:5d} batches | lr {lr:.2e} | ms/batch {ms_per_batch:5.2f} | loss {loss_value:5.2f} | bpc {bpc:8.4f}\")\n",
        "            print(f\"| epoch {epoch} | {batch_idx + 1:5d}/{num_batches:<5d} batches | lr {lr:.2e} | ms/batch {ms_per_batch:5.2f} | loss {loss_value:5.2f} | bpc {bpc:8.4f}\")\n",
        "            log_message( f\"| epoch {epoch} | {batch_idx + 1:5d}/{num_batches:<5d} batches | lr {lr:.2e} | ms/batch {ms_per_batch:5.2f} | loss {loss_value:5.2f} | bpc {bpc:8.4f}\", log_filename)\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, vocab):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in progress_bar:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "            # Ensure SOS, EOS, and PAD tokens are on the correct device\n",
        "            tgt_input = pad_sequence([torch.cat((torch.tensor([vocab['[SOS]']]).to(device), x), dim=0) for x in tgt_input],\n",
        "                                     batch_first=True, padding_value=vocab['[PAD]']).to(device)\n",
        "            tgt_output = pad_sequence([torch.cat((x, torch.tensor([vocab['[EOS]']]).to(device)), dim=0) for x in tgt_output],\n",
        "                                     batch_first=True, padding_value=vocab['[PAD]']).to(device)\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            # Make sure the output and target are in the correct shape for the loss function\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), tgt_output.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def calculate_bpc(loss):\n",
        "    # Convert loss (cross entropy) to bits-per-character\n",
        "    return loss / np.log(2)\n",
        "\n",
        "def save_model(model, filepath):\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "    logging.info(f'Model saved to {filepath}')\n",
        "\n",
        "def load_model(model, filepath):\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    model.eval()\n",
        "    logging.info(f'Model loaded from {filepath}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy_Lcy_qCYUO"
      },
      "source": [
        "# Main Training Loop\n",
        "Specify data location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk2ASNWpMSfL",
        "outputId": "40611c92-83f6-4599-ecda-a9871fcb4729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Check if running on Google Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = '/content/drive/My Drive/LMDatasets/'\n",
        "else:\n",
        "    base_path = './LMDatasets/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jXhcK7gWdDCz"
      },
      "outputs": [],
      "source": [
        "def log_message(message, log_file):\n",
        "  with open(log_file, 'a') as file:\n",
        "    file.write(message+'\\n')\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Change attention type to change  type and  activate  layer normalisation or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oaEwh_YCCdzV"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Hyperparameters and file paths\n",
        "    train_file = os.path.join(base_path, 'nchlt_text.nr.train')\n",
        "    val_file = os.path.join(base_path, 'nchlt_text.nr.valid')\n",
        "    test_file = os.path.join(base_path, 'nchlt_text.nr.test')\n",
        "    seq_len = 128\n",
        "    batch_size = 128\n",
        "    log_interval = 200\n",
        "    epochs = 10\n",
        "    learning_rate = 1e-2\n",
        "    dropout = 0.1\n",
        "    max_data_size = None# Set this to None to load all data or specify a limit\n",
        "    layer_norm_after= True  # Set to True to apply layer normalization after residual connections\n",
        "    attention_type = 'default'  # Options: 'default', 'multi-query', 'sparse'\n",
        "    block_size = 128  # Used for sparse attention\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Prepare data\n",
        "    train_dataset, val_dataset, test_dataset, vocab = prepare_datasets(train_file, val_file, test_file, seq_len, max_data_size)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=8)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=8)\n",
        "\n",
        "    # Initialize model, criterion, and optimizer\n",
        "    model = TransformerModel(vocab_size=len(vocab), dropout=dropout, layer_norm_after=layer_norm_after, attention_type=attention_type, block_size=block_size).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab['[PAD]'])\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * epochs)  # Set T_max based on total batches\n",
        "\n",
        "   # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3  # Early stopping patience\n",
        "    no_improvement = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(\"Start Training\")\n",
        "        logging.info(\"Start Training\")\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, vocab, epoch, log_interval)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device, vocab)\n",
        "        train_bpc = calculate_bpc(train_loss)\n",
        "        val_bpc = calculate_bpc(val_loss)\n",
        "\n",
        "        # Log training progress\n",
        "        #print(f'| end of epoch {epoch} | valid loss {val_loss:.4f} | valid ppl {val_bpc:.4f}')\n",
        "        #logging.info(f'| end of epoch {epoch} | valid loss {val_loss:.4f} | valid ppl {val_bpc:.4f}')\n",
        "        log_message(f'| end of epoch {epoch} | valid loss {val_loss:.4f} | valid ppl {val_bpc:.4f}', log_filename)\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement = 0\n",
        "            save_model(model, f'model_epoch_{epoch}.pt')\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "            if no_improvement >= patience:\n",
        "                logging.info(f'Early stopping at epoch {epoch}')\n",
        "                break\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss = evaluate(model, test_loader, criterion, device, vocab)\n",
        "    test_bpc = calculate_bpc(test_loss)\n",
        "\n",
        "    print(f'| End of training | test loss {test_loss:.4f} | test ppl {test_bpc:.4f}')\n",
        "    logging.info(f'| End of training | test loss {test_loss:.4f} | test ppl {test_bpc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVSMwq7DChuQ",
        "outputId": "7a517a74-b6c1-4a62-f04c-f5d7ea2b93ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1:   0%|          | 0/1171 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Training Epoch 1:  17%|█▋        | 200/1171 [05:27<26:56,  1.67s/it, bpc=617, loss=428]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 1 |   200/1171  batches | lr 1.00e-02 | ms/batch 1638.01 | loss 427.96 | bpc 617.4182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1:  34%|███▍      | 400/1171 [10:59<21:22,  1.66s/it, bpc=311, loss=216]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 1 |   400/1171  batches | lr 1.00e-02 | ms/batch 1659.74 | loss 215.83 | bpc 311.3815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1:  51%|█████     | 600/1171 [16:31<15:49,  1.66s/it, bpc=209, loss=145]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 1 |   600/1171  batches | lr 1.00e-02 | ms/batch 1660.42 | loss 144.93 | bpc 209.0895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1:  68%|██████▊   | 800/1171 [22:03<10:14,  1.66s/it, bpc=158, loss=109]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 1 |   800/1171  batches | lr 1.00e-02 | ms/batch 1660.27 | loss 109.48 | bpc 157.9433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1:  85%|████████▌ | 1000/1171 [27:35<04:44,  1.66s/it, bpc=127, loss=88.2]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 1 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1659.98 | loss 88.21 | bpc 127.2546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2:  17%|█▋        | 200/1171 [05:32<26:52,  1.66s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 2 |   200/1171  batches | lr 1.00e-02 | ms/batch 1663.64 | loss  3.12 | bpc   4.4996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2:  34%|███▍      | 400/1171 [11:05<21:29,  1.67s/it, bpc=16.6, loss=11.5]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 2 |   400/1171  batches | lr 1.00e-02 | ms/batch 1661.41 | loss 11.48 | bpc  16.5687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2:  51%|█████     | 600/1171 [16:36<15:48,  1.66s/it, bpc=14.7, loss=10.2]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 2 |   600/1171  batches | lr 1.00e-02 | ms/batch 1659.48 | loss 10.21 | bpc  14.7267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2:  68%|██████▊   | 800/1171 [22:09<10:16,  1.66s/it, bpc=12.2, loss=8.44]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 2 |   800/1171  batches | lr 1.00e-02 | ms/batch 1661.24 | loss  8.44 | bpc  12.1822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2:  85%|████████▌ | 1000/1171 [27:40<04:44,  1.66s/it, bpc=10.6, loss=7.38]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 2 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1657.64 | loss  7.38 | bpc  10.6451\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3:  17%|█▋        | 200/1171 [05:32<26:53,  1.66s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 3 |   200/1171  batches | lr 1.00e-02 | ms/batch 1660.46 | loss  3.12 | bpc   4.4959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3:  34%|███▍      | 400/1171 [11:03<21:20,  1.66s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 3 |   400/1171  batches | lr 1.00e-02 | ms/batch 1658.30 | loss  3.12 | bpc   4.4950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3:  51%|█████     | 600/1171 [16:35<15:50,  1.66s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 3 |   600/1171  batches | lr 1.00e-02 | ms/batch 1659.75 | loss  3.12 | bpc   4.4953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3:  68%|██████▊   | 800/1171 [22:07<10:17,  1.67s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 3 |   800/1171  batches | lr 1.00e-02 | ms/batch 1659.47 | loss  3.12 | bpc   4.4951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3:  85%|████████▌ | 1000/1171 [27:39<04:44,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 3 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1657.65 | loss  3.12 | bpc   4.4949\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4:  17%|█▋        | 200/1171 [05:32<26:48,  1.66s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 4 |   200/1171  batches | lr 1.00e-02 | ms/batch 1662.63 | loss  3.12 | bpc   4.4961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4:  34%|███▍      | 400/1171 [11:04<21:20,  1.66s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 4 |   400/1171  batches | lr 1.00e-02 | ms/batch 1658.76 | loss  3.12 | bpc   4.4956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4:  51%|█████     | 600/1171 [16:36<15:48,  1.66s/it, bpc=4.5, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 4 |   600/1171  batches | lr 1.00e-02 | ms/batch 1658.68 | loss  3.12 | bpc   4.4953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4:  68%|██████▊   | 800/1171 [22:07<10:16,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 4 |   800/1171  batches | lr 1.00e-02 | ms/batch 1659.42 | loss  3.12 | bpc   4.4950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4:  85%|████████▌ | 1000/1171 [27:39<04:45,  1.67s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 4 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1659.23 | loss  3.12 | bpc   4.4948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5:  17%|█▋        | 200/1171 [05:31<26:50,  1.66s/it, bpc=34.5, loss=23.9]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 5 |   200/1171  batches | lr 1.00e-02 | ms/batch 1657.95 | loss 23.92 | bpc  34.5124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5:  34%|███▍      | 400/1171 [11:03<21:19,  1.66s/it, bpc=21.4, loss=14.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 5 |   400/1171  batches | lr 1.00e-02 | ms/batch 1658.45 | loss 14.84 | bpc  21.4165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5:  51%|█████     | 600/1171 [16:34<15:46,  1.66s/it, bpc=15.8, loss=10.9]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 5 |   600/1171  batches | lr 1.00e-02 | ms/batch 1658.24 | loss 10.94 | bpc  15.7759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5:  68%|██████▊   | 800/1171 [22:06<10:13,  1.65s/it, bpc=13, loss=8.98]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 5 |   800/1171  batches | lr 1.00e-02 | ms/batch 1657.31 | loss  8.98 | bpc  12.9554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5:  85%|████████▌ | 1000/1171 [27:37<04:43,  1.66s/it, bpc=11.3, loss=7.81]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 5 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1657.10 | loss  7.81 | bpc  11.2632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6:  17%|█▋        | 200/1171 [05:31<26:54,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 6 |   200/1171  batches | lr 1.00e-02 | ms/batch 1658.84 | loss  3.12 | bpc   4.4945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6:  34%|███▍      | 400/1171 [11:03<21:18,  1.66s/it, bpc=4.49, loss=3.11]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 6 |   400/1171  batches | lr 1.00e-02 | ms/batch 1657.81 | loss  3.11 | bpc   4.4938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6:  51%|█████     | 600/1171 [16:34<15:49,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 6 |   600/1171  batches | lr 1.00e-02 | ms/batch 1657.21 | loss  3.12 | bpc   4.4943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6:  68%|██████▊   | 800/1171 [22:06<10:16,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 6 |   800/1171  batches | lr 1.00e-02 | ms/batch 1657.48 | loss  3.12 | bpc   4.4941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6:  85%|████████▌ | 1000/1171 [27:38<04:44,  1.66s/it, bpc=4.49, loss=3.11]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 6 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1659.31 | loss  3.11 | bpc   4.4938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7:  17%|█▋        | 200/1171 [05:32<26:47,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 7 |   200/1171  batches | lr 1.00e-02 | ms/batch 1660.73 | loss  3.12 | bpc   4.4946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7:  34%|███▍      | 400/1171 [11:03<21:20,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 7 |   400/1171  batches | lr 1.00e-02 | ms/batch 1657.95 | loss  3.12 | bpc   4.4945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7:  51%|█████     | 600/1171 [16:35<15:50,  1.66s/it, bpc=4.49, loss=3.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 7 |   600/1171  batches | lr 1.00e-02 | ms/batch 1658.67 | loss  3.12 | bpc   4.4941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7:  68%|██████▊   | 800/1171 [22:07<10:16,  1.66s/it, bpc=4.49, loss=3.11]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 7 |   800/1171  batches | lr 1.00e-02 | ms/batch 1657.68 | loss  3.11 | bpc   4.4939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7:  85%|████████▌ | 1000/1171 [27:38<04:44,  1.66s/it, bpc=9.08, loss=6.29]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 7 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1655.73 | loss  6.29 | bpc   9.0780\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8:  17%|█▋        | 200/1171 [05:31<26:51,  1.66s/it, bpc=5.12, loss=3.55]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 8 |   200/1171  batches | lr 1.00e-02 | ms/batch 1659.88 | loss  3.55 | bpc   5.1184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8:  34%|███▍      | 400/1171 [11:03<21:22,  1.66s/it, bpc=4.81, loss=3.33]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 8 |   400/1171  batches | lr 1.00e-02 | ms/batch 1655.55 | loss  3.33 | bpc   4.8066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8:  51%|█████     | 600/1171 [16:34<15:42,  1.65s/it, bpc=4.7, loss=3.26]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 8 |   600/1171  batches | lr 1.00e-02 | ms/batch 1654.79 | loss  3.26 | bpc   4.7015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8:  68%|██████▊   | 800/1171 [22:05<10:12,  1.65s/it, bpc=4.65, loss=3.22]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 8 |   800/1171  batches | lr 1.00e-02 | ms/batch 1656.65 | loss  3.22 | bpc   4.6497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8:  85%|████████▌ | 1000/1171 [27:36<04:44,  1.66s/it, bpc=4.62, loss=3.2]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch 8 |  1000/1171  batches | lr 1.00e-02 | ms/batch 1657.76 | loss  3.20 | bpc   4.6184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| End of training | test loss 3.1467 | test ppl 4.5397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #log_message('Start', log_filename)\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
